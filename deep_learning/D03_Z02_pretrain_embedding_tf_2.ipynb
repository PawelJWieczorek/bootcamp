{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "print(tf.__version__)\n",
    "\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, SimpleRNN, LSTM, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import History\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n",
    "\n",
    "It must specify 3 arguments:\n",
    "\n",
    "* **input_dim**: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "* **output_dim**: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "* **input_length**: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n",
    "\n",
    "# Zad. \n",
    "Podążamy za stroną: \n",
    "\n",
    "https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n",
    "\n",
    "mamy jakiś zbiór tekstów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "docs = ['Well done!',\n",
    "    'Good work',\n",
    "    'Great effort',\n",
    "    'nice work',\n",
    "    'Excellent!',\n",
    "    'Weak',\n",
    "    'Poor effort!',\n",
    "    'not good',\n",
    "    'poor work',\n",
    "    'Could have done better.']\n",
    "# define class labels\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Do warstwy **Embedding layer** wchodzi sekwencja intów.\n",
    "\n",
    "* my wykorzystamy reprezenatację Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39, 31], [41, 34], [30, 42], [5, 34], [28], [11], [47, 42], [3, 41], [47, 34], [31, 17, 31, 33]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "vocab_size = 50\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekwencje mają różne długości, a Keras wymaga aby wejścia były równej długość."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39 31  0  0]\n",
      " [41 34  0  0]\n",
      " [30 42  0  0]\n",
      " [ 5 34  0  0]\n",
      " [28  0  0  0]\n",
      " [11  0  0  0]\n",
      " [47 42  0  0]\n",
      " [ 3 41  0  0]\n",
      " [47 34  0  0]\n",
      " [31 17 31 33]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Embeding ma rakres 50 i długość wejściową 4. Zmniejszmy embending do wymiaru 8.\n",
    "* Model jest prostym klasyfikatorem binarnym. \n",
    "* Co ważne, wynik z warstwy Embeding będzie wynosił 4 wektory o 8 wymiarach każdy, po jednym dla każdego słowa. \n",
    "* Spłaszczamy to do jednego 32-elementowego wektora, aby przejść do warstwy wyjściowej Dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 4, 8)              400       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 433\n",
      "Trainable params: 433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "history_1 = History()\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 0.6878 - accuracy: 0.5556 - val_loss: 0.6765 - val_accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6864 - accuracy: 0.5556 - val_loss: 0.6766 - val_accuracy: 1.0000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6850 - accuracy: 0.5556 - val_loss: 0.6767 - val_accuracy: 1.0000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6836 - accuracy: 0.5556 - val_loss: 0.6767 - val_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6822 - accuracy: 0.5556 - val_loss: 0.6768 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6808 - accuracy: 0.5556 - val_loss: 0.6769 - val_accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6794 - accuracy: 0.5556 - val_loss: 0.6770 - val_accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6780 - accuracy: 0.5556 - val_loss: 0.6770 - val_accuracy: 1.0000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6767 - accuracy: 0.5556 - val_loss: 0.6771 - val_accuracy: 1.0000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6753 - accuracy: 0.5556 - val_loss: 0.6772 - val_accuracy: 1.0000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6739 - accuracy: 0.5556 - val_loss: 0.6772 - val_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.6725 - accuracy: 0.5556 - val_loss: 0.6773 - val_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6711 - accuracy: 0.5556 - val_loss: 0.6774 - val_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.6697 - accuracy: 0.5556 - val_loss: 0.6774 - val_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6684 - accuracy: 0.5556 - val_loss: 0.6775 - val_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.6670 - accuracy: 0.5556 - val_loss: 0.6775 - val_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6656 - accuracy: 0.5556 - val_loss: 0.6775 - val_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6642 - accuracy: 0.5556 - val_loss: 0.6776 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6628 - accuracy: 0.5556 - val_loss: 0.6776 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.6614 - accuracy: 0.5556 - val_loss: 0.6776 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.6600 - accuracy: 0.5556 - val_loss: 0.6776 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 0.6586 - accuracy: 0.5556 - val_loss: 0.6776 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6572 - accuracy: 0.5556 - val_loss: 0.6776 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6558 - accuracy: 0.5556 - val_loss: 0.6775 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.6544 - accuracy: 0.5556 - val_loss: 0.6775 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6530 - accuracy: 0.5556 - val_loss: 0.6775 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6516 - accuracy: 0.5556 - val_loss: 0.6774 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.6502 - accuracy: 0.6667 - val_loss: 0.6774 - val_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.6487 - accuracy: 0.6667 - val_loss: 0.6773 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6473 - accuracy: 0.6667 - val_loss: 0.6772 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.6459 - accuracy: 0.6667 - val_loss: 0.6771 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6444 - accuracy: 0.6667 - val_loss: 0.6770 - val_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6430 - accuracy: 0.6667 - val_loss: 0.6769 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6415 - accuracy: 0.6667 - val_loss: 0.6768 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6400 - accuracy: 0.6667 - val_loss: 0.6767 - val_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.6385 - accuracy: 0.6667 - val_loss: 0.6765 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.6371 - accuracy: 0.6667 - val_loss: 0.6764 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 0.6356 - accuracy: 0.6667 - val_loss: 0.6762 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.6341 - accuracy: 0.6667 - val_loss: 0.6761 - val_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6325 - accuracy: 0.6667 - val_loss: 0.6759 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.6310 - accuracy: 0.6667 - val_loss: 0.6757 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.6295 - accuracy: 0.6667 - val_loss: 0.6755 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6279 - accuracy: 0.6667 - val_loss: 0.6753 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6264 - accuracy: 0.6667 - val_loss: 0.6751 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6248 - accuracy: 0.6667 - val_loss: 0.6749 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 0.6233 - accuracy: 0.6667 - val_loss: 0.6746 - val_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.6217 - accuracy: 0.6667 - val_loss: 0.6744 - val_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6201 - accuracy: 0.6667 - val_loss: 0.6742 - val_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6185 - accuracy: 0.6667 - val_loss: 0.6739 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6169 - accuracy: 0.6667 - val_loss: 0.6736 - val_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6153 - accuracy: 0.6667 - val_loss: 0.6734 - val_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6136 - accuracy: 0.6667 - val_loss: 0.6731 - val_accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.6120 - accuracy: 0.6667 - val_loss: 0.6728 - val_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.6103 - accuracy: 0.6667 - val_loss: 0.6725 - val_accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.6087 - accuracy: 0.6667 - val_loss: 0.6722 - val_accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6070 - accuracy: 0.7778 - val_loss: 0.6719 - val_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.6053 - accuracy: 0.7778 - val_loss: 0.6716 - val_accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6036 - accuracy: 0.7778 - val_loss: 0.6713 - val_accuracy: 1.0000\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step - loss: 0.6019 - accuracy: 0.8889 - val_loss: 0.6710 - val_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6002 - accuracy: 0.8889 - val_loss: 0.6707 - val_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5985 - accuracy: 0.8889 - val_loss: 0.6704 - val_accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5968 - accuracy: 0.8889 - val_loss: 0.6700 - val_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.5951 - accuracy: 0.8889 - val_loss: 0.6697 - val_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.5933 - accuracy: 0.8889 - val_loss: 0.6694 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 0.5916 - accuracy: 0.8889 - val_loss: 0.6690 - val_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.5898 - accuracy: 0.8889 - val_loss: 0.6687 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5880 - accuracy: 0.8889 - val_loss: 0.6684 - val_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5862 - accuracy: 0.8889 - val_loss: 0.6680 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5844 - accuracy: 0.8889 - val_loss: 0.6677 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5826 - accuracy: 0.8889 - val_loss: 0.6673 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.5808 - accuracy: 0.8889 - val_loss: 0.6670 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5790 - accuracy: 0.8889 - val_loss: 0.6666 - val_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5772 - accuracy: 0.8889 - val_loss: 0.6663 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5754 - accuracy: 0.8889 - val_loss: 0.6659 - val_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5735 - accuracy: 0.8889 - val_loss: 0.6656 - val_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.5717 - accuracy: 0.8889 - val_loss: 0.6652 - val_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5698 - accuracy: 0.8889 - val_loss: 0.6649 - val_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5679 - accuracy: 0.8889 - val_loss: 0.6645 - val_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5661 - accuracy: 0.8889 - val_loss: 0.6641 - val_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.5642 - accuracy: 0.8889 - val_loss: 0.6638 - val_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5623 - accuracy: 0.8889 - val_loss: 0.6634 - val_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.5604 - accuracy: 0.8889 - val_loss: 0.6631 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5585 - accuracy: 0.8889 - val_loss: 0.6627 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.5566 - accuracy: 0.8889 - val_loss: 0.6624 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5547 - accuracy: 0.8889 - val_loss: 0.6620 - val_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5527 - accuracy: 0.8889 - val_loss: 0.6616 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5508 - accuracy: 0.8889 - val_loss: 0.6613 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5489 - accuracy: 0.88 - 0s 51ms/step - loss: 0.5489 - accuracy: 0.8889 - val_loss: 0.6609 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5469 - accuracy: 0.8889 - val_loss: 0.6606 - val_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.5450 - accuracy: 0.8889 - val_loss: 0.6602 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.5430 - accuracy: 0.8889 - val_loss: 0.6599 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5410 - accuracy: 0.8889 - val_loss: 0.6595 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5391 - accuracy: 0.8889 - val_loss: 0.6592 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5371 - accuracy: 0.8889 - val_loss: 0.6588 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5351 - accuracy: 0.8889 - val_loss: 0.6585 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.5331 - accuracy: 0.8889 - val_loss: 0.6581 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5312 - accuracy: 0.8889 - val_loss: 0.6578 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5292 - accuracy: 0.8889 - val_loss: 0.6575 - val_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5272 - accuracy: 0.8889 - val_loss: 0.6571 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5252 - accuracy: 0.8889 - val_loss: 0.6568 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x799c0e288>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # compile the model\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# # fit the model\n",
    "# model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(padded_docs, labels, epochs=100, validation_split=0.1, callbacks=[history_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.999998\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_1.history['accuracy'], label = \"tarina Adam\")\n",
    "plt.plot(history_1.history['val_accuracy'], label = \"test Adam\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain embedding\n",
    "\n",
    "https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "\n",
    "* GloVe embedding data can be found at: http://nlp.stanford.edu/data/glove.6B.zip (source page: http://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "* After downloading and unzipping, you will see a few files, one of which is “glove.6B.50d.txt“, which contains a 100-dimensional version of the embedding.\n",
    "\n",
    "\n",
    "Pojedyńczy plik można pobrać z tąd:\n",
    "https://www.dropbox.com/sh/tjq47ybybgnrbel/AAAVbp0UkQTAbKWVMIi5mtHpa?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B.50d')\n",
    "\n",
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "# file = open(filename, encoding=\"utf8\")\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt'), encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.1891e-01,  1.5255e-01, -8.2073e-02, -7.4144e-01,  7.5917e-01,\n",
       "       -4.8328e-01, -3.1009e-01,  5.1476e-01, -9.8708e-01,  6.1757e-04,\n",
       "       -1.5043e-01,  8.3770e-01, -1.0797e+00, -5.1460e-01,  1.3188e+00,\n",
       "        6.2007e-01,  1.3779e-01,  4.7108e-01, -7.2874e-02, -7.2675e-01,\n",
       "       -7.4116e-01,  7.5263e-01,  8.8180e-01,  2.9561e-01,  1.3548e+00,\n",
       "       -2.5701e+00, -1.3523e+00,  4.5880e-01,  1.0068e+00, -1.1856e+00,\n",
       "        3.4737e+00,  7.7898e-01, -7.2929e-01,  2.5102e-01, -2.6156e-01,\n",
       "       -3.4684e-01,  5.5841e-01,  7.5098e-01,  4.9830e-01, -2.6823e-01,\n",
       "       -2.7443e-03, -1.8298e-02, -2.8096e-01,  5.5318e-01,  3.7706e-02,\n",
       "        1.8555e-01, -1.5025e-01, -5.7512e-01, -2.6671e-01,  9.2121e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index[\"i\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "docs = ['Well done!',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!',\n",
    "        'Weak',\n",
    "        'Poor effort!',\n",
    "        'not good',\n",
    "        'poor work',\n",
    "        'Could have done better.']\n",
    "# define class labels\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides a Tokenizer class that can be fit on the training data, can convert text to sequences consistently by calling the texts_to_sequences() method on the Tokenizer class, and provides access to the dictionary mapping of words to integers in a word_index attribute.\n",
    "\n",
    "https://keras.io/preprocessing/text/#tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 2], [3, 1], [7, 4], [8, 1], [9], [10], [5, 4], [11, 3], [5, 1], [12, 13, 2, 14]]\n"
     ]
    }
   ],
   "source": [
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekwencje mają różne długości, a Keras wymaga aby wejścia były równej długość."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6  2  0  0]\n",
      " [ 3  1  0  0]\n",
      " [ 7  4  0  0]\n",
      " [ 8  1  0  0]\n",
      " [ 9  0  0  0]\n",
      " [10  0  0  0]\n",
      " [ 5  4  0  0]\n",
      " [11  3  0  0]\n",
      " [ 5  1  0  0]\n",
      " [12 13  2 14]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create a matrix of one embedding for each word in the training dataset. We can do that by enumerating all unique words in the Tokenizer.word_index and locating the embedding weight vector from the loaded GloVe embedding.\n",
    "\n",
    "The result is a matrix of weights only for words we will see during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 50)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 50))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "embedding_matrix.shape        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.13589978e-01,  1.96950004e-01, -5.19439995e-01, -8.62179995e-01,\n",
       "        1.54940002e-02,  1.09729998e-01, -8.02929997e-01, -3.33609998e-01,\n",
       "       -1.61189993e-04,  1.01889996e-02,  4.67340015e-02,  4.67510015e-01,\n",
       "       -4.74750012e-01,  1.10380001e-01,  3.93269986e-01, -4.36520010e-01,\n",
       "        3.99839997e-01,  2.71090001e-01,  4.26499993e-01, -6.06400013e-01,\n",
       "        8.11450005e-01,  4.56299990e-01, -1.27260000e-01, -2.24739999e-01,\n",
       "        6.40709996e-01, -1.27670002e+00, -7.22310007e-01, -6.95900023e-01,\n",
       "        2.80450005e-02, -2.30719998e-01,  3.79959989e+00, -1.26249999e-01,\n",
       "       -4.79669988e-01, -9.99719977e-01, -2.19760001e-01,  5.05649984e-01,\n",
       "        2.59530004e-02,  8.05140018e-01,  1.99290007e-01,  2.87959993e-01,\n",
       "       -1.59150004e-01, -3.04380000e-01,  1.60249993e-01, -1.82899997e-01,\n",
       "       -3.85629982e-02, -1.76190004e-01,  2.70409994e-02,  4.68420014e-02,\n",
       "       -6.28970027e-01,  3.57259989e-01])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.13589978e-01,  1.96950004e-01, -5.19439995e-01, -8.62179995e-01,\n",
       "        1.54940002e-02,  1.09729998e-01, -8.02929997e-01, -3.33609998e-01,\n",
       "       -1.61189993e-04,  1.01889996e-02,  4.67340015e-02,  4.67510015e-01,\n",
       "       -4.74750012e-01,  1.10380001e-01,  3.93269986e-01, -4.36520010e-01,\n",
       "        3.99839997e-01,  2.71090001e-01,  4.26499993e-01, -6.06400013e-01,\n",
       "        8.11450005e-01,  4.56299990e-01, -1.27260000e-01, -2.24739999e-01,\n",
       "        6.40709996e-01, -1.27670002e+00, -7.22310007e-01, -6.95900023e-01,\n",
       "        2.80450005e-02, -2.30719998e-01,  3.79959989e+00, -1.26249999e-01,\n",
       "       -4.79669988e-01, -9.99719977e-01, -2.19760001e-01,  5.05649984e-01,\n",
       "        2.59530004e-02,  8.05140018e-01,  1.99290007e-01,  2.87959993e-01,\n",
       "       -1.59150004e-01, -3.04380000e-01,  1.60249993e-01, -1.82899997e-01,\n",
       "       -3.85629982e-02, -1.76190004e-01,  2.70409994e-02,  4.68420014e-02,\n",
       "       -6.28970027e-01,  3.57259989e-01])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key difference is that the embedding layer can be seeded with the GloVe word embedding weights. We chose the 50-dimensional version, therefore the Embedding layer must be defined with output_dim set to 50. Finally, we do not want to update the learned word weights in this model, therefore we will set the trainable attribute for the model to be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Embedding(vocab_size, 50, weights=[embedding_matrix], input_length=4, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 4, 50)             750       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 951\n",
      "Trainable params: 201\n",
      "Non-trainable params: 750\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.callbacks import History\n",
    "\n",
    "history_2 = History()\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, 50, weights=[embedding_matrix], input_length=4, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 0.6000 - accuracy: 0.6667 - val_loss: 1.3902 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5944 - accuracy: 0.6667 - val_loss: 1.3763 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5889 - accuracy: 0.6667 - val_loss: 1.3627 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5835 - accuracy: 0.6667 - val_loss: 1.3496 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5782 - accuracy: 0.6667 - val_loss: 1.3372 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5729 - accuracy: 0.6667 - val_loss: 1.3255 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5678 - accuracy: 0.6667 - val_loss: 1.3143 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5627 - accuracy: 0.7778 - val_loss: 1.3035 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5577 - accuracy: 0.7778 - val_loss: 1.2931 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5528 - accuracy: 0.7778 - val_loss: 1.2831 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5479 - accuracy: 0.7778 - val_loss: 1.2735 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5432 - accuracy: 0.7778 - val_loss: 1.2641 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5384 - accuracy: 0.7778 - val_loss: 1.2551 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5338 - accuracy: 0.7778 - val_loss: 1.2464 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5292 - accuracy: 0.7778 - val_loss: 1.2380 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5247 - accuracy: 0.7778 - val_loss: 1.2299 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5203 - accuracy: 0.8889 - val_loss: 1.2221 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5159 - accuracy: 0.8889 - val_loss: 1.2146 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5116 - accuracy: 0.8889 - val_loss: 1.2074 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5074 - accuracy: 0.8889 - val_loss: 1.2006 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5032 - accuracy: 0.8889 - val_loss: 1.1940 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4990 - accuracy: 0.8889 - val_loss: 1.1878 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4949 - accuracy: 0.8889 - val_loss: 1.1819 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4909 - accuracy: 0.8889 - val_loss: 1.1763 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4869 - accuracy: 0.8889 - val_loss: 1.1709 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4830 - accuracy: 0.8889 - val_loss: 1.1659 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4791 - accuracy: 0.8889 - val_loss: 1.1611 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4752 - accuracy: 0.8889 - val_loss: 1.1566 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4714 - accuracy: 0.8889 - val_loss: 1.1524 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4676 - accuracy: 0.8889 - val_loss: 1.1484 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4639 - accuracy: 0.8889 - val_loss: 1.1446 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4602 - accuracy: 0.8889 - val_loss: 1.1411 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4566 - accuracy: 0.8889 - val_loss: 1.1378 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4530 - accuracy: 0.8889 - val_loss: 1.1347 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4494 - accuracy: 0.8889 - val_loss: 1.1318 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4459 - accuracy: 0.8889 - val_loss: 1.1291 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4424 - accuracy: 0.8889 - val_loss: 1.1265 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4389 - accuracy: 0.8889 - val_loss: 1.1241 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4355 - accuracy: 0.8889 - val_loss: 1.1219 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4321 - accuracy: 0.8889 - val_loss: 1.1198 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4288 - accuracy: 0.8889 - val_loss: 1.1178 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4255 - accuracy: 0.8889 - val_loss: 1.1160 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4222 - accuracy: 0.8889 - val_loss: 1.1143 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4189 - accuracy: 1.0000 - val_loss: 1.1127 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4157 - accuracy: 1.0000 - val_loss: 1.1112 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4126 - accuracy: 1.0000 - val_loss: 1.1098 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4094 - accuracy: 1.0000 - val_loss: 1.1084 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4063 - accuracy: 1.0000 - val_loss: 1.1072 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4033 - accuracy: 1.0000 - val_loss: 1.1060 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4002 - accuracy: 1.0000 - val_loss: 1.1049 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3972 - accuracy: 1.0000 - val_loss: 1.1039 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3942 - accuracy: 1.0000 - val_loss: 1.1030 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3913 - accuracy: 1.0000 - val_loss: 1.1020 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3884 - accuracy: 1.0000 - val_loss: 1.1012 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3855 - accuracy: 1.0000 - val_loss: 1.1004 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3827 - accuracy: 1.0000 - val_loss: 1.0997 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3798 - accuracy: 1.0000 - val_loss: 1.0990 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3771 - accuracy: 1.0000 - val_loss: 1.0983 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3743 - accuracy: 1.0000 - val_loss: 1.0977 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3716 - accuracy: 1.0000 - val_loss: 1.0972 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3689 - accuracy: 1.0000 - val_loss: 1.0967 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3662 - accuracy: 1.0000 - val_loss: 1.0962 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3635 - accuracy: 1.0000 - val_loss: 1.0958 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3609 - accuracy: 1.0000 - val_loss: 1.0954 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3583 - accuracy: 1.0000 - val_loss: 1.0951 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3558 - accuracy: 1.0000 - val_loss: 1.0948 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3532 - accuracy: 1.0000 - val_loss: 1.0946 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3507 - accuracy: 1.0000 - val_loss: 1.0944 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3482 - accuracy: 1.0000 - val_loss: 1.0943 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3458 - accuracy: 1.0000 - val_loss: 1.0943 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3433 - accuracy: 1.0000 - val_loss: 1.0943 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3409 - accuracy: 1.0000 - val_loss: 1.0943 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3385 - accuracy: 1.0000 - val_loss: 1.0944 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3361 - accuracy: 1.0000 - val_loss: 1.0945 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3338 - accuracy: 1.0000 - val_loss: 1.0947 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3315 - accuracy: 1.0000 - val_loss: 1.0950 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3292 - accuracy: 1.0000 - val_loss: 1.0953 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3269 - accuracy: 1.0000 - val_loss: 1.0957 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3247 - accuracy: 1.0000 - val_loss: 1.0961 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3224 - accuracy: 1.0000 - val_loss: 1.0966 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3202 - accuracy: 1.0000 - val_loss: 1.0971 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3180 - accuracy: 1.0000 - val_loss: 1.0977 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3159 - accuracy: 1.0000 - val_loss: 1.0984 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3137 - accuracy: 1.0000 - val_loss: 1.0991 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3116 - accuracy: 1.0000 - val_loss: 1.0998 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3095 - accuracy: 1.0000 - val_loss: 1.1006 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3074 - accuracy: 1.0000 - val_loss: 1.1015 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3053 - accuracy: 1.0000 - val_loss: 1.1024 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3033 - accuracy: 1.0000 - val_loss: 1.1034 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3013 - accuracy: 1.0000 - val_loss: 1.1044 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2993 - accuracy: 1.0000 - val_loss: 1.1054 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2973 - accuracy: 1.0000 - val_loss: 1.1065 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2953 - accuracy: 1.0000 - val_loss: 1.1077 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2934 - accuracy: 1.0000 - val_loss: 1.1089 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2915 - accuracy: 1.0000 - val_loss: 1.1101 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2895 - accuracy: 1.0000 - val_loss: 1.1114 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2876 - accuracy: 1.0000 - val_loss: 1.1127 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2858 - accuracy: 1.0000 - val_loss: 1.1140 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2839 - accuracy: 1.0000 - val_loss: 1.1154 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2821 - accuracy: 1.0000 - val_loss: 1.1169 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7b3f16908>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(padded_docs, labels, epochs=100, validation_split=0.1, callbacks=[history_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.999998\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de3hV1Z3/8ffKBSIQLhLQQIDgDHIxgRBDCDcLikQogqKDoNTLtKJStE4VEU1RsbW0dBzrA+ooCDpS1KmD+LMRKRWK1AICouUe0NwIQhJNCEmAXNbvj5OchlzICTnhcPb5vJ6Hh+x99t5nbXb4ZGWdtb/bWGsRERH/F+TrBoiIiHco0EVEHEKBLiLiEAp0ERGHUKCLiDhEiK/eOCIiwkZHR/vq7UVE/NKOHTvyrLVd6nvNZ4EeHR3N9u3bffX2IiJ+yRiT0dBrGnIREXEIBbqIiEMo0EVEHEKBLiLiEAp0ERGHaDTQjTGvG2OOG2N2N/C6Mca8aIw5ZIz5yhgT7/1miohIYzzpoa8AbjjH6+OBPlV/ZgIvN79ZIiLSVI3OQ7fWbjLGRJ9jk8nAm9ZVh3eLMaajMSbSWnvUS20820ePw7f/aJFDi/85jeUtiig1lb5uiojHRl8aS8ykV7x+XG/cWNQdyKqxnF21rk6gG2Nm4urF07NnTy+8tQS6LZzihaACAIxK+4uf6FpeSEwLHNcbgW7qWVfvfy1r7avAqwAJCQnn999v/MLz2k2cKWPPm7B9EZunbaZD6w6+bo6IT3ljlks20KPGchSQ44XjijQqsyiT8FbhCnMRvBPoHwB3Vs12SQIKW2z8XKSWrKIseoZr+E4EPBhyMcasAkYDEcaYbOApIBTAWvsKkApMAA4BJcA9LdVYkdoyT2QSGxHr62aIXBQ8meUyvZHXLfBTr7VIxENlFWXkFOcw4YoJvm6KyEVBd4qK38opzqHSVmrIRaSKAl38VlaRa7Zsz/YKdBFQoIsfyzyRCUCP8B6NbCkSGBTo4reyirK4JOQSOod19nVTRC4KCnTxW5lFmfQM74kx9d3bJhJ4FOjitzJPZGr8XKQGBbr4pYrKCrJPZmv8XKQGBbr4pWMlxyivLNeURZEaFOjilzKLNMNFpDYFuvil6imLGkMX+ScFuvilrKIsWgW1omubrr5uishFQ4EufinzRCY9wnsQZPQtLFJN/xvEL2UWZdKjvcbPRWpSoIvfsdaSXZStGS4itSjQxe/kluZyquKUZriI1OKNZ4pKE5WWl7J893JKy0t93RS/lFeaB+DXPfTKSssrmw5TUFLm66aIDyRfdTlX9+rk9eMq0H3gsyOf8fKXL9M6uLU+1DtPXdt0pV/nfr5uxnnbe/QEv117gFYhQQSrFk3AuSKirQLdKapvivlk6ie0b9Xex60RX0jPLwZgzU9H0D9S3wPiHeoe+kBmUSadWndSmAewjPwSAHp1buPjloiTKNB9IKsoS1PuAlxGfjFdwlvTppV+SRbvUaD7QNaJLM3QCHDp+SVEq3cuXqZAv8DOVJzhaPFRv56hIc2XkV9Mr85tfd0McRgF+gWWfTIbi1UPPYCVnCnn2InT6qGL1ynQL7CsE3pSfaDL/K76A1H10MW7FOgXWPWURQ25BK70PM1wkZahQL/AsoqyCA8Np2Prjr5uivhI5neuOei9LlUPXbxLgX6BZRZlEhUepSfVB7D0/BI6tQmlQ5tQXzdFHEaBfoFlncjS+HmA0wwXaSkK9AuorLKMnJM5Gj8PcOl5moMuLUOBfgF9e/Jbym25piwGsNPlFeQUltJTPXRpAQr0C8g9w0VDLgEr67tSrEU9dGkRHgW6MeYGY8wBY8whY8zj9bze0xizwRjzhTHmK2PMBO831f9lFVXNQdeQS8Byz3BRD11aQKOBbowJBpYA44EBwHRjzIBam6UA71prBwPTgJe83VAnyCzK5JKQS4i4JMLXTREfqZ6Drh66tARPeuiJwCFr7dfW2jPA28DkWttYoLoWbAcgx3tNdI6sE1mashjgMvKLCW8dwqVtW/m6KeJAngR6dyCrxnJ21bqangZmGGOygVTgwfoOZIyZaYzZbozZnpubex7N9W+ZRZkabglw6fkl9OzcRj/UpUV4Euj1fefZWsvTgRXW2ihgAvA/xtR9tpq19lVrbYK1NqFLly5Nb60fq6isIKsoS4Ee4DLyi4nW+Lm0EE8CPRuoOc8uirpDKj8G3gWw1v4dCAM0UFzD8ZLjlFWW6cEWAay8opLs70tVw0VajCePS/kc6GOM6Q0cwfWh5+21tskErgNWGGP64wp0vxpTee/ge+z/bn+LHT//VD6gGS6BILfoNK/89TBlFZVnrS89U0F5pVUPXVpMo4FurS03xswGPgaCgdettXuMMQuA7dbaD4BHgNeMMf+Bazjmbmtt7WGZi1Z5ZTm/3PJLQoJCCAsJa7H36dW+F/0u9d8n1YtnUv9xlGWbv6Fjm9A645XdOoRxdbT3n/YuAp710LHWpuL6sLPmuvk1vt4LjPBu0y6co8VHKbflzB86n5v73Ozr5oifS88vpk2rYL74xfX68FMuKN0pyj8fOqFb8sUbMvJL6NW5rcJcLjgFOrolX7wrPb9YNw6JTyjQcd2SHxYcRpdLAmsqpXhfRaUl+7tS3dovPqFAx9VD79G+h35FlmY7WljKmYpK9dDFJxTouMbQe7TT+Lk0X0a+q1ZLTwW6+EDAB3qlrXTdwanxc/GC9HxXNUXNNRdfCPhAP15ynDOVZzTDRbwiI7+EViFBXN6+5e5nEGlIwAd65gnNcBHvSc8rptelbQgK0ucxcuEp0KunLOqWfPGC6jnoIr4Q8IGeVZRFaFAol7W5zNdNET9nrSXjO81BF99RoBdl0b1dd4KDgn3dFPFzx4tOc6qsUtUUxWcCPtAzT2Rq/Fy8Ij1PzwsV3wroQLfW6ilC4jXVc9A1ZVF8JaADPf9UPqXlpZqyKF6Rnl9MSJChW0dNWRTfCOhA15RF8aaM/BJ6XNqGkOCA/m8lPhTQ33lZRa6yuRpyEW/I+K5YH4iKTwV0oGcWZRJsgolsF+nrpoifs9aSkVdCr0sV6OI7AR3oWSeyiGwbSWhQqK+bIn7uu+IzFJ0u1wwX8amADvTMIk1ZFO9Ir57hEqEeuviOR88UvZh8mv0p6zLWeeVYhwsOM/lfJ3vlWP4oPa+Y/950mPIKv3me90Ur+/tSQHPQxbf8LtCPFh9ly9EtXjlW50s6c03UNV45lj/6v53ZrNqWRbcOmmbnDQm9OtFTY+jiQ34X6FP7TmVq36m+boYjpOeX0PPSNmx6bIyvmyIiXuB3gS7ek5GvaXa+VlZWRnZ2NqdOnfJ1U+QiExYWRlRUFKGhnk/aUKAHsPT8Em4cpCmbvpSdnU14eDjR0dF6pq24WWvJz88nOzub3r17e7xfQM9yCWQFJWcoLC1T3REfO3XqFJ07d1aYy1mMMXTu3LnJv7kp0ANU9TQ7zcrwPYW51Od8vi8U6AEqw/0wY42hB7KCggJeeuml89p3woQJFBQUNOv9ly9fTlxcHHFxcbRq1YrY2Fji4uJ4/PHHPT5GVlYWt91223m3ISoqqt7zSElJ4YUXXjjv4/qCxtADVEZ+CcZAD02zC2jVgT5r1iyP97HWYq0lNTW12e9/zz33cM899wAQHR3Nhg0biIiIqLNdeXk5ISH1x1WPHj145513mt0WJ1APPUCl5xdzefswwkL1pKZA9vjjj3P48GHi4uKYM2cOJ0+e5LrrriM+Pp7Y2FjWrFkDQHp6Ov3792fWrFnEx8eTlZVFdHQ0eXl57tfuvfderrrqKsaNG0dpqetGq9dee40hQ4YwaNAgbrnlFkpKSjxuW0pKCvfddx/XX38999xzD4cPH2bUqFEMHjyYq6++mq1btwJw6NAh4uLiAFi6dCm33norycnJ9OnTh3nz5rmPN3PmTBISErjqqqtYsGDBWe+1cOFCEhMTGTp0KF9//XWdtqSlpZGcnMzVV1/NNddcw8GDB5v2D32BqIceoFwPM1bv/GLyzP/bw96cE1495oBu7XnqxqsafH3hwoXs3r2bXbt2Aa6e8OrVq2nfvj15eXkkJSUxadIkAA4cOMDy5cvrHaJJS0tj1apVvPbaa0ydOpX33nuPGTNmMGXKFO69917AFdDLli3jwQcf9Lj9X3zxBZs2bSIsLIySkhL+/Oc/ExYWxv79+7nrrrvcoV7Tl19+yc6dOwkJCeHKK6/kwQcfpFu3bixcuJBLL72U8vJyxowZw6233sqAAQMA6NSpE9u2beP111/n5z//Oe+///5Zx5w5cyZLly7lX/7lX/jb3/7G7NmzWbfOO3ese5NHgW6MuQH4PRAMLLXWLqxnm6nA04AFvrTW3u7FdoqXZeQXM7a/HowtZ7PW8sQTT7Bp0yaCgoI4cuQIx44dA6BXr14kJSXVu1/v3r3dveSrr76a9PR0AHbv3k1KSgoFBQWcPHmS5OTkJrVn8uTJhIW57mQ+ffo0s2fP5ssvvyQkJITDhw/Xu8/YsWMJDw8HoF+/fmRmZtKtWzdWrVrFsmXLKC8vJycnh71797oDffr06QDccccddcbvCwoK2LJlC7fccot7XXl5eZPO40JpNNCNMcHAEuB6IBv43BjzgbV2b41t+gDzgBHW2u+NMV1bqsHSfCdPl5N38oxmuFxkztWTvlBWrlxJbm4uO3bsIDQ0lOjoaPfUubZtG/5+ad26tfvr4OBg95DL3Xffzfvvv8+gQYNYsWIFGzdubFJ7ar7nf/7nf9KjRw/eeustysrKaNeunUdtKS8vJy0tjd///vds27aNjh07MmPGjLOmBJ5rRom1loiICPdvMRczT8bQE4FD1tqvrbVngLeB2hWt7gWWWGu/B7DWHvduM8WbNMNFqoWHh1NUVOReLiwspGvXroSGhrJhwwYyMjKadfyioiIiIyMpKytj5cqVzTpWYWEhkZGRGGN44403sNbzonInTpwgPDyc9u3bc/ToUT7++OOzXq/+UHXVqlWMGDHirNc6depEZGQkq1evBqCyspIvv/yyWefSUjwZcukOZNVYzgaG1trmSgBjzN9wDcs8ba1dW/tAxpiZwEyAnj1VttZXMjQHXap07tyZESNGEBMTw/jx45k7dy433ngjCQkJxMXF0a9fv2Yd/9lnn2Xo0KH06tWL2NjYs354NNXs2bO59dZbWbVqFWPHjj2rJ96Y+Ph4BgwYQExMDFdccUWd0C4pKSExMRFjDKtWraqz/9tvv80DDzzA008/zZkzZ5gxYwaDBg0673NpKaaxn3LGmH8Dkq21P6la/hGQaK19sMY2HwJlwFQgCvgUiLHWNjhJNSEhwW7fvr35ZyBN9tLGQ/x27QF2P5NMu9b6XNyX9u3bR//+/X3dDLlI1ff9YYzZYa1NqG97T4ZcsoEeNZajgJx6tlljrS2z1n4DHAD6eNxquaAy8kqIaNdaYS7iMJ4E+udAH2NMb2NMK2Aa8EGtbd4HxgAYYyJwDcHUncwpF4X0/GKNn4s4UKOBbq0tB2YDHwP7gHettXuMMQuMMZOqNvsYyDfG7AU2AHOstfkt1WhpHtccdI2fiziNR79zW2tTgdRa6+bX+NoCP6/6IxexU2UVfHvilHroIg6kW/8DTOZ3VTNcItRDF3EaBXqASc9zzUHvpaJcIo7jd9Mc1u89xpova0+yEU/986Yi9dDFdVv7H/7whyZVW6zphRdeYObMmbRpc3YH4eabb+abb77h5MmT5Obmup+689JLLzF8+HCPjr1kyRI6duzIHXfc0eR2rV+/nsWLF9epyQKucrm7d++mY8eOTT7uxc7vAj3v5Gn2HCn0dTP82g1XXU6HNp4/p1Cc63zK59b0wgsvMGPGjDqBXn1X5caNG/nd737Hhx9+WO/+5yqL+9Of/vS82hTI/C7QpyX2ZFqi7jIV8Yaa5XOvv/56Fi1axKJFi3j33Xc5ffo0N998M8888wzFxcVMnTqV7OxsKioq+MUvfsGxY8fIyclhzJgxREREsGHDBo/eMyoqivvuu4+1a9fy8MMPk5+fz7Jlyzhz5gxXXnklb775JpdccgkpKSlERETw8MMPM3LkSEaOHMknn3xCYWEhy5cvZ/jw4Rw+fJi7776bkydPEhQUxEsvvcTQoa4b2QsLC7nppps4ePAgY8aMYfHixXVqtrzxxhssWbKEM2fOMHz4cBYvXkxQkP+ORPtdoIs41kePw7f/8O4xL4+F8XWKo7rVLp+7bt060tLS2LZtG9ZaJk2axKZNm8jNzaVbt2786U9/Alxh2aFDB55//vkGH0pxLm3btuVvf/sbAPn5+dx///2A6wfMihUreOCBB+rsY61l27ZtfPDBByxYsIC1a9cSGRnZYEndrVu3snfvXnr06MH111/PmjVruOmmm9zH2717N6tXr+azzz4jJCSEmTNn8vbbb3P77f5bKFaBLiJu69atY926dQwePBiAkydPkpaWxqhRo3j00UeZO3cuEydOZNSoUc16n5qPjPvqq6+YP38+BQUFFBUVMXHixHr3mTJlCnB2ed5zldRNSkoiOjoagGnTprF58+azAn39+vV8/vnnJCS47qIvLS2lR4+aN8X7HwW6yMXiHD3pC8Vay7x587jvvvvqvLZjxw5SU1OZN28e48aNY/78+fUcwTM1y+LeeeedfPTRR8TExLB06VK2bNlS7z7VxbiqS+LCuUvq1h5eqb1sreXf//3fefbZZ8/7PC42/jtYJCLNVrt8bnJyMq+//jonT54E4MiRIxw/fpycnBzatGnDjBkzePTRR9m5c2e9+5+P4uJiLr/8csrKyvjDH/7QpH3PVVJ3y5YtZGZmUlFRwbvvvsvIkSPP2nfs2LG8++675OXlAa6hn8zMzGadi6+phy4SwGqXz120aBH79u1j2LBhALRr14633nqLQ4cOMWfOHIKCgggNDeXll18GXI9mGz9+PJGRkR5/KFrbggULSExMpGfPnsTExJz14InGnKuk7vDhw3nkkUfYs2cPo0ePdj9Kr1psbCxPPfUUY8eOpbKyktDQUF555RW/Lu3daPnclqLyuSIqnyvn1hLlc0VExA8o0EVEHEKBLiLiEAp0ERGHUKCLiDiEAl1ExCEU6CIBrLra4vmYMGECBQUFzW7DihUrCAoK4quvvnKvi4mJcd/e35Dnnnuu3vVDhw4lLi6Onj170qVLF+Li4oiLi2v0eDU9+eST5z2vfunSpTz88MN11peXl7d4yV4FukgAO59At9ZSWVlJamqq1wIqKiqKX/3qV03ap6FA37p1K7t27WLBggXcdttt7Nq1i127drnrulSrqKho8Ni/+tWvGDNmTJPaczFQoIsEsJrlc+fMmcPJkye57rrriI+PJzY2ljVr1gCQnp5O//79mTVrFvHx8WRlZREdHU1eXp77tXvvvZerrrqKcePGUVpaCsBrr73GkCFDGDRoELfccgslJSX1tmPixIns2bOHAwcO1Hlt1apVxMbGEhMTw9y5c93tLi0tJS4uzuMHYFT3kFNSUkhMTGTbtm089dRTDBkyhJiYGO6//3536YAZM2a4H44RFRXF008/zeDBgxk4cCAHDx4EXKUFhg0bxuDBgxkxYgRpaWnu98rIyCA5OZm+ffvyy1/+st72LFy4kMTERAYOHMiCBQs8OofG6NZ/kYvEb7b9hv3f7ffqMftd2o+5iXMbfL12+dzy8nJWr15N+/btycvLIykpyX3L/IEDB1i+fHm9Pfq0tDRWrVrFa6+9xtSpU3nvvfeYMWMGU6ZM4d577wUgJSWFZcuW8eCDD9bZPygoiMcee4znnnuON954w70+JyeHuXPnsmPHDjp16sS4ceN4//33WbhwIYsXL3a321OFhYXEx8e7Q7Zv374888wzWGu5/fbbWbt2LePHj6+z32WXXcYXX3zBiy++yPPPP88rr7xC//792bx5M8HBwaxdu5aUlBTeeecdALZt28bu3btp1aoVQ4YMYeLEicTExLiPl5qaSmZmJlu3bsVay4QJE/jss888fppTQ9RDFxE3ay1PPPEEAwcOZOzYsRw5coRjx44B0KtXL5KSkurdr3fv3sTFxQFnl7fdvXs3o0aNIjY2lpUrV7Jnz54G3/v2229ny5YtfPPNN+51n3/+OaNHj6ZLly6EhIRwxx13sGnTpvM+v1atWnHzzTe7l//yl7+QmJjIoEGD+Otf/9pg++or3VtQUMCUKVOIiYnh0UcfPWvf5ORkOnXqRNu2bbnpppvYvHnzWcdbt24dH330EYMHDyY+Pp5Dhw65e/7NoR66yEXiXD3pC2XlypXk5uayY8cOQkNDiY6OdhfLqlnytraaRbGCg4PdQy53330377//PoMGDWLFihVs3LixwWOEhITwyCOP8Jvf/Ma9ztu1pi655BJ3Gd2SkhJmz57Nzp076d69OykpKQ0WBquvdO+TTz5JcnIys2bN4tChQ9xwww3u7T0p3ZuSksKPf/xjr50bqIcuEtBql78tLCyka9euhIaGsmHDBjIyMpp1/KKiIiIjIykrK2PlypWNbn/33Xezfv16cnNzAdeMlb/+9a/k5eVRUVHBqlWr+MEPfgBAaGgoZWVl59220tJSgoKCiIiIoKioiPfee69J+xcWFtK9e3fANVOnpnXr1lFQUEBJSQlr1qxhxIgRZ72enJzMsmXLKC52PbQ9OzvbXca3ORToIgGsZvncOXPmcMcdd7B9+3YSEhJYuXIl/fr1a9bxn332WYYOHcr111/v0bFatWrFQw89xPHjxwGIjIzk17/+NWPGjGHQoEHEx8czefJkwFW6d+DAgR5/KFpb586dueuuu4iJieHmm292P4vUU3PnzmXOnDl1whpg5MiR3H777QwePJjp06e7h6OqTZgwgVtvvZWkpCRiY2OZOnWquwZ9c6h8rogPqXyunIvK54qIBCgFuoiIQyjQRUQcQoEuIuIQCnQREYfwKNCNMTcYYw4YYw4ZYx4/x3a3GmOsMabeT2BFRKTlNBroxphgYAkwHhgATDfGDKhnu3DgIWCrtxspIi2jOeVzAV544YUGC26NHj2ahIR/9u22b9/O6NGjz3m8Xbt2kZqaWmf9xx9/7C6D265dO/r27UtcXBx33nmnx22tqKhg1KhRHm9f28iRI+utHdNQuVxf8KSHnggcstZ+ba09A7wNTK5nu2eB3wL13zsrIhedlgx0gOPHj/PRRx95fLyGAj05OdldBrf6pqddu3bx5ptvnrVd9W359QkODubTTz/1uC3+yJNA7w5k1VjOrlrnZowZDPSw1n54rgMZY2YaY7YbY7ZX39orIr5Tu3wuwKJFixgyZAgDBw7kqaeeAqC4uJgf/vCHDBo0iJiYGN555x1efPFFcnJyGDNmTIO1w+fMmVNv+dhTp05xzz33EBsby+DBg9mwYQNnzpxh/vz5vPPOO8TFxbkrFzZm6dKlTJs2jYkTJzJ+/HhOnDjBtddeS3x8PAMHDuTDD12xVPMBE+vXr+e6665jypQp9O3b96yefkMldcF1i/+wYcOIjY2lvhsjjx07xpQpU0hISCAxMZEtW7Z4dA7e4klxLlPPOvcZGmOCgP8C7m7sQNbaV4FXwXWnqGdNFAkM3z73HKf3ebd8buv+/bj8iScafL12+dx169aRlpbGtm3bsNYyadIkNm3aRG5uLt26deNPf/oT4Kpj0qFDB55//nk2bNhAREREvccfNmwYq1evZsOGDYSHh7vXL1myBIB//OMf7N+/n3HjxnHw4EEWLFjA9u3bWbx4cZPO8+9//zu7du2iU6dOlJWVsWbNGsLDwzl+/DgjRoxg4sSJdfbZuXMne/fupWvXriQlJbFlyxaSkpL42c9+1mBJ3dOnT/P3v/+dTz75hJ/85Cd1hmAeeughHnvsMZKSkkhPT2fixIns3r27SefSHJ700LOBHjWWo4CcGsvhQAyw0RiTDiQBH+iDURH/s27dOtatW+cu67p//37S0tKIjY1l/fr1zJ07l08//ZQOHTp4fMyUlJQ6vfTNmzfzox/9CIB+/frRq1evZpWPHTduHJ06dQJclQznzp3LwIEDGTduHFlZWfUWvkpKSiIyMpLg4OCzHlF3rpK606dPB+Daa6/l+PHjdeqvrF+/nvvvv5+4uDhuuukmvv/+e3flyQvBkx7650AfY0xv4AgwDbi9+kVrbSHg/vFsjNkIPGqtVaEWkSY4V0/6QrHWMm/ePO677746r+3YsYPU1FTmzZvHuHHjmD9/vkfHvPbaa/nFL35x1vCDt2tI1Szt++abb1JYWMjOnTsJCQkhKiqq3rK4tUv+lpeXN1pS15OyuNu2baNVq1beOrUmabSHbq0tB2YDHwP7gHettXuMMQuMMZNauoEi0nJql89NTk7m9ddfd/c8jxw5wvHjx8nJyaFNmzbMmDGDRx99lJ07d9a7f0OefPJJfvvb37qXr7nmGnc53YMHD5KZmUnfvn09Pt65VJcADgkJ4c9//jNHjhzxeN/GSupWj+tv3LiRyy67rE6N+LFjx7qHk4AmP1GpuTx6wIW1NhVIrbWu3h/P1trRzW+WiFwINcvnjh8/nkWLFrFv3z6GDRsGQLt27Xjrrbc4dOgQc+bMISgoiNDQUF5++WXAVcJ2/PjxREZGsmHDhgbfZ8KECXTp0sW9PGvWLO6//35iY2MJCQlhxYoVtG7dmjFjxrBw4ULi4uKYN28et912W5PP6Uc/+hE33ngjCQkJxMfH06dPnyb9e1SX1O3Vq1edkrrt27dn+PDhFBUVsXz58jr7L1myhAceeIDly5dTXl7OmDFjzgr4lqbyuSI+pPK5ci4qnysiEqAU6CIiDqFAFxFxCAW6iI/56nMsubidz/eFAl3Eh8LCwsjPz1eoy1msteTn5xMWFtak/TyatigiLSMqKors7GxU20hqCwsLIyoqqkn7KNBFfCg0NJTevXv7uhniEBpyERFxCAW6iIhDKNBFRBxCgS4i4hAKdBERh1Cgi4g4hAJdRMQhFOgiIg6hQBcRcQgFuoiIQyjQRUQcQoEuIuIQCnQREYdQoIuIOIQCXUTEIRToIiIOoUAXEXEIBbqIiEMo0EVEHEKBLiLiEAp0ERGHUJOCueUAAAclSURBVKCLiDiER4FujLnBGHPAGHPIGPN4Pa//3Biz1xjzlTHmL8aYXt5vqoiInEujgW6MCQaWAOOBAcB0Y8yAWpt9ASRYawcCfwR+6+2GiojIuXnSQ08EDllrv7bWngHeBibX3MBau8FaW1K1uAWI8m4zRUSkMZ4Eencgq8ZydtW6hvwY+Ki+F4wxM40x240x23Nzcz1vpYiINMqTQDf1rLP1bmjMDCABWFTf69baV621CdbahC5dunjeShERaVSIB9tkAz1qLEcBObU3MsaMBZ4EfmCtPe2d5omIiKc86aF/DvQxxvQ2xrQCpgEf1NzAGDMY+G9gkrX2uPebKSIijWk00K215cBs4GNgH/CutXaPMWaBMWZS1WaLgHbA/xpjdhljPmjgcCIi0kI8GXLBWpsKpNZaN7/G12O93C4REWki3SkqIuIQCnQREYdQoIuIOIQCXUTEIRToIiIOoUAXEXEIBbqIiEMo0EVEHEKBLiLiEAp0ERGHUKCLiDiEAl1ExCEU6CIiDqFAFxFxCAW6iIhDKNBFRBxCgS4i4hAKdBERh1Cgi4g4hAJdRMQhFOgiIg6hQBcRcQgFuoiIQyjQRUQcQoEuIuIQCnQREYdQoIuIOIQCXUTEIRToIiIOoUAXEXEIBbqIiEN4FOjGmBuMMQeMMYeMMY/X83prY8w7Va9vNcZEe7uhIiJybo0GujEmGFgCjAcGANONMQNqbfZj4Htr7b8C/wX8xtsNFRGRcwvxYJtE4JC19msAY8zbwGRgb41tJgNPV339R2CxMcZYa60X2wrAt889x+l9+719WBGRC6Z1/35c/sQTXj+uJ0Mu3YGsGsvZVevq3cZaWw4UAp1rH8gYM9MYs90Ysz03N/f8WiwiIvXypIdu6llXu+ftyTZYa18FXgVISEg4r957S/xUExFxAk966NlAjxrLUUBOQ9sYY0KADsB33migiIh4xpNA/xzoY4zpbYxpBUwDPqi1zQfAXVVf3wp80hLj5yIi0rBGh1ysteXGmNnAx0Aw8Lq1do8xZgGw3Vr7AbAM+B9jzCFcPfNpLdloERGpy5MxdKy1qUBqrXXza3x9Cvg37zZNRESaQneKiog4hAJdRMQhFOgiIg6hQBcRcQjjq9mFxphcIOM8d48A8rzYHH8RiOcdiOcMgXnegXjO0PTz7mWt7VLfCz4L9OYwxmy31ib4uh0XWiCedyCeMwTmeQfiOYN3z1tDLiIiDqFAFxFxCH8N9Fd93QAfCcTzDsRzhsA870A8Z/DiefvlGLqIiNTlrz10ERGpRYEuIuIQfhfojT2w2gmMMT2MMRuMMfuMMXuMMT+rWn+pMebPxpi0qr87+bqt3maMCTbGfGGM+bBquXfVg8fTqh5E3srXbfQ2Y0xHY8wfjTH7q675sAC51v9R9f292xizyhgT5rTrbYx53Rhz3Bizu8a6eq+tcXmxKtu+MsbEN/X9/CrQPXxgtROUA49Ya/sDScBPq87zceAv1to+wF+qlp3mZ8C+Gsu/Af6r6py/x/VAcqf5PbDWWtsPGITr/B19rY0x3YGHgARrbQyu0tzTcN71XgHcUGtdQ9d2PNCn6s9M4OWmvplfBTo1HlhtrT0DVD+w2lGstUettTurvi7C9R+8O65zfaNqszeAm3zTwpZhjIkCfggsrVo2wLW4HjwOzjzn9sA1uJ4pgLX2jLW2AIdf6yohwCVVTzlrAxzFYdfbWruJuk9va+jaTgbetC5bgI7GmMimvJ+/BbonD6x2FGNMNDAY2ApcZq09Cq7QB7r6rmUt4gXgMaCyarkzUFD14HFw5vW+AsgFllcNNS01xrTF4dfaWnsE+B2QiSvIC4EdOP96Q8PXttn55m+B7tHDqJ3CGNMOeA942Fp7wtftaUnGmInAcWvtjpqr69nUadc7BIgHXrbWDgaKcdjwSn2qxo0nA72BbkBbXEMOtTntep9Ls7/f/S3QPXlgtSMYY0JxhflKa+3/Va0+Vv0rWNXfx33VvhYwAphkjEnHNZR2La4ee8eqX8nBmdc7G8i21m6tWv4jroB38rUGGAt8Y63NtdaWAf8HDMf51xsavrbNzjd/C3RPHljt96rGjpcB+6y1z9d4qebDuO8C1lzotrUUa+08a22UtTYa13X9xFp7B7AB14PHwWHnDGCt/RbIMsb0rVp1HbAXB1/rKplAkjGmTdX3e/V5O/p6V2no2n4A3Fk12yUJKKwemvGYtdav/gATgIPAYeBJX7enhc5xJK5ftb4CdlX9mYBrTPkvQFrV35f6uq0tdP6jgQ+rvr4C2AYcAv4XaO3r9rXA+cYB26uu9/tAp0C41sAzwH5gN/A/QGunXW9gFa7PCMpw9cB/3NC1xTXksqQq2/6BawZQk95Pt/6LiDiEvw25iIhIAxToIiIOoUAXEXEIBbqIiEMo0EVEHEKBLiLiEAp0ERGH+P8GTV2absRrSwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_1.history['accuracy'], label = \"tarina Trainable\")\n",
    "plt.plot(history_1.history['val_accuracy'], label = \"test Trainable\")\n",
    "\n",
    "plt.plot(history_2.history['accuracy'], label = \"tarina Not Trainable\")\n",
    "plt.plot(history_2.history['val_accuracy'], label = \"test Not Trainable\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
