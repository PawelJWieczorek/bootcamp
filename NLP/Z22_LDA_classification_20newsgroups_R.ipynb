{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import xgboost\n",
    "except ImportError as ex:\n",
    "    print(\"Error: the xgboost library is not installed.\")\n",
    "    xgboost = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "\n",
    "\n",
    "to_remove= ('headers', 'footers', 'quotes')\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='all', shuffle=True, remove = to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad\n",
    "\n",
    "Znajdź najlepszy model dla 20newsgroups wykonując GridSearch dla modeli:\n",
    "\n",
    "* MultinomialNB (bez redukcji wymiarowości)\n",
    "* LogisticRegression\n",
    "* LinearSVC\n",
    "* SVC\n",
    "* KNeighborsClassifier\n",
    "* DecisionTreeClassifier\n",
    "* RandomForestClassifier\n",
    "* BaggingClassifier\n",
    "* ExtraTreesClassifier\n",
    "* AdaBoostClassifier\n",
    "* GradientBoostingClassifier\n",
    "* VotingClassifier\n",
    "* xgboost.XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups_train.data, newsgroups_train.target, test_size=0.25, random_state=33)\n",
    "\n",
    "plt.hist(y_train, alpha=0.5)\n",
    "plt.hist(y_test, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "seed=123\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=0.5, max_features=None,\n",
       "                 min_df=10, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True, stop_words='english', strip_accents='unicode',\n",
       "                 sublinear_tf=False, token_pattern='\\\\b[a-zA-Z]{3,}\\\\b',\n",
       "                 tokenizer=None, use_idf=True, vocabulary=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "    \n",
    "pipe = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)]\n",
    "}\n",
    "\n",
    "grid_0 = GridSearchCV(pipe, param_grid, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_0.fit(X_train, y_train)\n",
    "grid_0.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()), \n",
    "    (\"pca\", TruncatedSVD(n_components=10)), \n",
    "    ('classifier', LinearSVC(C=1))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],\n",
    "            'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid_1 = GridSearchCV(pipe, param_grid, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_1.fit(X_train, y_train)\n",
    "grid_1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_2 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()), \n",
    "    (\"pca\", TruncatedSVD(n_components=10)),\n",
    "    ('classifier', SVC(C=1, probability=True))\n",
    "])\n",
    "\n",
    "param_grid_2 = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],   \n",
    "            'classifier__C': [0.01, 0.1, 1, 10, 100,],\n",
    "            'classifier__gamma': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid_2 = GridSearchCV(pipe_2, param_grid_2, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_2.fit(X_train, y_train)\n",
    "grid_2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__C': 100,\n",
       " 'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=0.5, max_features=None, min_df=10,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words='english', strip_accents='unicode', sublinear_tf=False,\n",
       "         token_pattern='\\\\b[a-zA-Z]{3,}\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipe_3 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()), \n",
    "    (\"pca\", TruncatedSVD(n_components=10)),\n",
    "    ('classifier', LogisticRegression(C=1))\n",
    "])\n",
    "\n",
    "param_grid_3 = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],   \n",
    "            'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid_3 = GridSearchCV(pipe_3, param_grid_3, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_3.fit(X_train, y_train)\n",
    "grid_3.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__metric': 'cosine',\n",
       " 'classifier__n_neighbors': 10,\n",
       " 'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=0.5, max_features=None, min_df=10,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words='english', strip_accents='unicode', sublinear_tf=False,\n",
       "         token_pattern='\\\\b[a-zA-Z]{3,}\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pipe_4 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    (\"pca\", TruncatedSVD(n_components=10)),\n",
    "    ('classifier', KNeighborsClassifier(n_neighbors=2, metric='euclidean'))\n",
    "])\n",
    "\n",
    "param_grid_4 = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],   \n",
    "            'classifier__n_neighbors': [2, 5, 10],\n",
    "            'classifier__metric': ['euclidean', 'cityblock', 'cosine']\n",
    "}\n",
    "\n",
    "\n",
    "grid_4 = GridSearchCV(pipe_4, param_grid_4, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_4.fit(X_train, y_train)\n",
    "grid_4.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__max_depth': 20,\n",
       " 'classifier__max_leaf_nodes': 20,\n",
       " 'classifier__min_samples_split': 10,\n",
       " 'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=0.5, max_features=None, min_df=10,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words='english', strip_accents='unicode', sublinear_tf=False,\n",
       "         token_pattern='\\\\b[a-zA-Z]{3,}\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipe_5 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    (\"pca\", TruncatedSVD(n_components=10)),\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "param_grid_5 = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],    \n",
    "            'classifier__max_depth': [5,10,20],\n",
    "            'classifier__min_samples_split': [5,10,20],\n",
    "            'classifier__max_leaf_nodes': [5,10,20]\n",
    "}\n",
    "\n",
    "\n",
    "grid_5 = GridSearchCV(pipe_5, param_grid_5, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_5.fit(X_train, y_train)\n",
    "grid_5.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__max_samples': 20,\n",
       " 'classifier__n_estimators': 100,\n",
       " 'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=0.5, max_features=None, min_df=10,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words='english', strip_accents='unicode', sublinear_tf=False,\n",
       "         token_pattern='\\\\b[a-zA-Z]{3,}\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None)}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipe_6 = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                   (\"pca\", TruncatedSVD(n_components=10)),\n",
    "                   ('classifier', BaggingClassifier(\n",
    "                                    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
    "                                    max_samples=100, bootstrap=True, random_state=42))\n",
    "                  ])\n",
    "\n",
    "param_grid_6 = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],    \n",
    "            'classifier__n_estimators': [10,50,100],\n",
    "            'classifier__max_samples': [10,20]\n",
    "             }\n",
    "\n",
    "grid_6 = GridSearchCV(pipe_6, param_grid_6, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_6.fit(X_train, y_train)\n",
    "grid_6.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__max_depth': 20,\n",
       " 'classifier__max_leaf_nodes': 20,\n",
       " 'classifier__n_estimators': 50,\n",
       " 'vectorizer': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=0.5, max_features=None, min_df=10,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words='english', strip_accents='unicode', sublinear_tf=False,\n",
       "         token_pattern='\\\\b[a-zA-Z]{3,}\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None)}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe_7 = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                   (\"pca\", TruncatedSVD(n_components=10)),\n",
    "                   ('classifier', RandomForestClassifier(n_estimators=500, max_leaf_nodes=16))\n",
    "                  ])\n",
    "\n",
    "param_grid_7 = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],    \n",
    "            'classifier__n_estimators': [10, 50, 100],\n",
    "            'classifier__max_leaf_nodes': [10, 20],\n",
    "            'classifier__max_depth': [10, 20]\n",
    "             }\n",
    "\n",
    "grid_7 = GridSearchCV(pipe_7, param_grid_7, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_7.fit(X_train, y_train)\n",
    "grid_7.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "pipe_8 = Pipeline([('vectorizer', CountVectorizer()), \n",
    "                   (\"pca\", TruncatedSVD(n_components=10)),\n",
    "                   ('classifier', ExtraTreesClassifier(n_estimators=500, max_leaf_nodes=16))\n",
    "                  ])\n",
    "\n",
    "param_grid_8 = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],\n",
    "                'classifier__n_estimators': [10, 50, 100],\n",
    "                'classifier__max_leaf_nodes': [10, 20],\n",
    "                'classifier__max_depth': [10, 20]\n",
    "             }\n",
    "\n",
    "grid_8 = GridSearchCV(pipe_8, param_grid_8, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_8.fit(X_train, y_train)\n",
    "grid_8.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipe_9 = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                   (\"pca\", TruncatedSVD(n_components=10)),\n",
    "                   ('classifier', AdaBoostClassifier(\n",
    "                        DecisionTreeClassifier(max_depth=1), \n",
    "                        n_estimators=1, learning_rate=0.5, \n",
    "                        algorithm=\"SAMME.R\", random_state=42)\n",
    "                   )\n",
    "                  ])\n",
    "\n",
    "\n",
    "param_grid_9 = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],\n",
    "                'classifier__n_estimators': [50, 100, 200],\n",
    "                'classifier__learning_rate': [0.1, 0.2,0.5,0.9, 1]\n",
    "             }\n",
    "\n",
    "grid_9 = GridSearchCV(pipe_9, param_grid_9, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_9.fit(X_train, y_train)\n",
    "grid_9.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "pipe_10 = Pipeline([('vectorizer', CountVectorizer()), \n",
    "                    (\"pca\", TruncatedSVD(n_components=10)),\n",
    "                   ('classifier', GradientBoostingClassifier(\n",
    "                       n_estimators=1, \n",
    "                      learning_rate=0.5, \n",
    "                      random_state=42))\n",
    "                  ])\n",
    "\n",
    "\n",
    "param_grid_10 = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],\n",
    "                'classifier__n_estimators': [50, 100, 200],\n",
    "                'classifier__learning_rate': [0.1, 0.2,0.5,0.9, 1]\n",
    "             }\n",
    "\n",
    "grid_10 = GridSearchCV(pipe_10, param_grid_10, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_10.fit(X_train, y_train)\n",
    "grid_10.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_11 = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                    (\"pca\", TruncatedSVD(n_components=10)),\n",
    "                   ('classifier', xgboost.XGBClassifier(n_estimators=1, \n",
    "                      learning_rate=0.5, \n",
    "                      random_state=42))\n",
    "                  ])\n",
    "\n",
    "\n",
    "param_grid_11 = {\n",
    "            'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10), \n",
    "                           CountVectorizer(strip_accents = 'unicode',\n",
    "                                stop_words = 'english',\n",
    "                                lowercase = True,\n",
    "                                token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                                max_df = 0.5, \n",
    "                                min_df = 10)],\n",
    "                'classifier__n_estimators': [50, 100, 200],\n",
    "                'classifier__learning_rate': [0.1, 0.2,0.5,0.9, 1]\n",
    "             }\n",
    "\n",
    "grid_11 = GridSearchCV(pipe_11, param_grid_11, cv=kfold, return_train_score=True)\n",
    "\n",
    "grid_11.fit(X_train, y_train)\n",
    "grid_11.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('grid_2', grid_2.best_estimator_), \n",
    "                ('grid_3', grid_3.best_estimator_), \n",
    "                ('grid_4', grid_4.best_estimator_), \n",
    "                ('grid_5', grid_5.best_estimator_), \n",
    "                ('grid_6', grid_6.best_estimator_), \n",
    "                ('grid_7', grid_7.best_estimator_), \n",
    "                ('grid_8', grid_8.best_estimator_), \n",
    "                ('grid_9', grid_9.best_estimator_),\n",
    "                ('grid_10', grid_10.best_estimator_), \n",
    "                ('grid_11', grid_11.best_estimator_)\n",
    "               ],\n",
    "    voting='soft')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "estimator = xgboost.XGBClassifier(n_jobs=-1)\n",
    "\n",
    "pipe_12 = Pipeline([('vectorizer', CountVectorizer()),\n",
    "                    (\"pca\", TruncatedSVD(n_components=10)),\n",
    "                   ('classifier', xgboost.XGBClassifier(n_jobs=-1))\n",
    "                  ])\n",
    "\n",
    "param_grid_12 = {\n",
    "    'vectorizer': [TfidfVectorizer(strip_accents = 'unicode',\n",
    "                        stop_words = 'english',\n",
    "                        lowercase = True,\n",
    "                        token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                        max_df = 0.5, \n",
    "                        min_df = 10), \n",
    "                   CountVectorizer(strip_accents = 'unicode',\n",
    "                        stop_words = 'english',\n",
    "                        lowercase = True,\n",
    "                        token_pattern = r'\\b[a-zA-Z]{3,}\\b',\n",
    "                        max_df = 0.5, \n",
    "                        min_df = 10)],\n",
    "    'classifier__max_depth': [3, 5, 8, 10],\n",
    "    'classifier__learning_rate': [0.001, 0.01, 0.05, 0.1],\n",
    "    'classifier__n_estimators': [50, 100, 150, 200, 400],\n",
    "    'classifier__gamma': [0, 0.5, 1, 2],\n",
    "    'classifier__colsample_bytree': [1, 0.8, 0.5],\n",
    "    'classifier__subsample': [1, 0.8, 0.5],\n",
    "    'classifier__min_child_weight': [1, 5, 10]\n",
    "}\n",
    "\n",
    "grid_12 = RandomizedSearchCV(n_iter=30,estimator=pipe_12, \n",
    "                             param_distributions=param_grid_12, \n",
    "                      cv=kfold, \n",
    "                      return_train_score=True)\n",
    "\n",
    "grid_12.fit(X_train, y_train)\n",
    "grid_12.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB\n",
      "precision_score: 0.7255156638785948\n",
      "recall_score: 0.7043718166383701\n",
      "f1_score: 0.6947302256516584\n",
      "accuracy_score: 0.7043718166383701\n",
      "SVM linear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision_score: 0.3700373476918\n",
      "recall_score: 0.39282682512733447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.34093662941938446\n",
      "accuracy_score: 0.39282682512733447\n"
     ]
    }
   ],
   "source": [
    "from sklearn import  metrics\n",
    "\n",
    "\n",
    "models = []\n",
    "models.append(('NB', grid_0.best_estimator_))\n",
    "models.append(('SVM linear', grid_1.best_estimator_))\n",
    "# models.append(('SVM rbf', grid_2.best_estimator_))\n",
    "# models.append(('LR', grid_3.best_estimator_))\n",
    "# models.append(('KNN', grid_4.best_estimator_))\n",
    "# models.append(('DecisionTreeClassifier', grid_5.best_estimator_))\n",
    "# models.append(('BaggingClassifier', grid_6.best_estimator_))\n",
    "# models.append(('RandomForestClassifier', grid_7.best_estimator_))\n",
    "# models.append(('ExtraTreesClassifier', grid_8.best_estimator_))\n",
    "# models.append(('AdaBoostClassifier', grid_9.best_estimator_))\n",
    "# models.append(('GradientBoostingClassifier', grid_10.best_estimator_))\n",
    "# models.append(('XGBClassifier', grid_11.best_estimator_))\n",
    "# models.append(('voting_clf', voting_clf))\n",
    "# models.append(('XGBClassifier r2', grid_12.best_estimator_))\n",
    "\n",
    "precision_score = []\n",
    "recall_score = []\n",
    "f1_score = []\n",
    "accuracy_score = []\n",
    "roc_auc_score = []\n",
    "for name, model in models:\n",
    "    print(name)\n",
    "    print(\"precision_score: {}\".format(metrics.precision_score(y_test , model.predict(X_test), average='weighted') ))\n",
    "    print(\"recall_score: {}\".format( metrics.recall_score(y_test , model.predict(X_test), average='weighted') ))\n",
    "    print(\"f1_score: {}\".format( metrics.f1_score(y_test , model.predict(X_test), average='weighted') ))\n",
    "    print(\"accuracy_score: {}\".format( metrics.accuracy_score(y_test , model.predict(X_test)) ))\n",
    "    \n",
    "\n",
    "    precision_score.append(metrics.precision_score(y_test , model.predict(X_test), average='weighted') )\n",
    "    recall_score.append(metrics.recall_score(y_test , model.predict(X_test), average='weighted') )\n",
    "    f1_score.append( metrics.f1_score(y_test , model.predict(X_test), average='weighted') )\n",
    "    accuracy_score.append(metrics.accuracy_score(y_test , model.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>accuracy_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NB</td>\n",
       "      <td>0.725516</td>\n",
       "      <td>0.704372</td>\n",
       "      <td>0.694730</td>\n",
       "      <td>0.704372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM linear</td>\n",
       "      <td>0.370037</td>\n",
       "      <td>0.392827</td>\n",
       "      <td>0.340937</td>\n",
       "      <td>0.392827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Method  precision_score  recall_score  f1_score  accuracy_score\n",
       "0          NB         0.725516      0.704372  0.694730        0.704372\n",
       "1  SVM linear         0.370037      0.392827  0.340937        0.392827"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "d = {'precision_score': precision_score, \n",
    "     'recall_score': recall_score, \n",
    "     'f1_score': f1_score,\n",
    "     'accuracy_score' : accuracy_score\n",
    "    }\n",
    "df = pd.DataFrame(data=d)\n",
    "df.insert(loc=0, column='Method', value=['NB', 'SVM linear'])\n",
    "# df.insert(loc=0, column='Method', value=['NB', 'SVM linear','SVM rbf','LR','KNN','DecisionTreeClassifier','BaggingClassifier','RandomForestClassifier','ExtraTreesClassifier', 'AdaBoostClassifier','GradientBoostingClassifier','XGBClassifier','voting','XGBClassifier r'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14134"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
