{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import fetch_mldata\n",
    "# mnist = fetch_mldata('MNIST original')\n",
    "# mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Import data\n",
    "# mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)\n",
    "\n",
    "# X = mnist.train.images # Returns np.array\n",
    "# y = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X/255.\n",
    "y = np.asarray(y, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(15, 8), subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for target, image, ax in zip(y, X, axes.ravel()):\n",
    "    ax.imshow(image.reshape(28, 28), cmap=plt.cm.gist_gray)\n",
    "    ax.set_title(target)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "some_digit = X[36000]\n",
    "some_digit_image = some_digit.reshape(28, 28)\n",
    "plt.imshow(some_digit_image, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(instances, images_per_row=10, **options):\n",
    "    size = 28\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary, **options)\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,9))\n",
    "example_images = np.r_[X[:12000:600], X[13000:30600:600], X[30600:60000:590]]\n",
    "plot_digits(example_images, images_per_row=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P0każmy przykładem binarnego klasyfikatora, zdolnego rozróżnić tylko dwie klasy:\n",
    " * 5 \n",
    "  nie-5. \n",
    "  \n",
    "Stwórzmy docelowe zboty (trin/test) dla tego zadania klasyfikacji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist.train.images \n",
    "y_train = mnist.train.labels\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = (y_train == 5)\n",
    "y_test_5 = (y_test == 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train_5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Zad. \n",
    "\n",
    "Stwórzmy **LogisticRegression** i naucz go na całym zbiorze treningowym.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz możesz go użyć aby wykryć czy dana liczba to  5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict([some_digit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasyfikator poprawnie wskazał, że ten obraz reprezentuje 5 (prawda). \n",
    "Wygląda na to, że odgadł w tym konkretnym przypadku! \n",
    "\n",
    "## Teraz oceńmy wydajność tego modelu.\n",
    "\n",
    "Wykorzystajmy funkcję \n",
    "```python\n",
    "cross_val_score() \n",
    "```\n",
    "do oceny naszego modelu **LogisticRegression** przy użyciu **K-fold crossvalidation**, z trzema fałdami. \n",
    "\n",
    "Pamiętaj, że **K-fold crossvalidation** oznacza podział zestawu treningowego na **K** części (w tym przypadku trzy), a następnie prognozowanie i ocenianie ich na każdym z kawałków przy użyciu modelu nauczonego na pozostałych danych (dwa pozostałe kawąłki)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Łał! Powyżej 97% dokładności (stosunek prawidłowych przewidywań) na wszystkich zbiorach walidacyjnych!**\n",
    "\n",
    "**To wygląda niesamowicie, prawda? **\n",
    "\n",
    "Zanim wpadniemy w samozachwyt :), spójrzmy na bardzo głupi klasyfikator, który klasyfikuje każdy pojedynczy obraz do klasy \"nie-5\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad.\n",
    "\n",
    "Zgadnij jakie **accuracy** dostaniemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "never_5_clf = Never5Classifier()\n",
    "cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Otrzymaliśmy ponad 90% dokładność! **\n",
    "\n",
    "Wynika to z faktu, że tylko około 10% obrazów to 5., więc jeśli zawsze zgadniesz, że obraz to nie 5, będziesz mieć rację w około 90% przypadków. \n",
    "\n",
    "To pokazuje dlaczego **accuracy** nie jest na ogół preferowaną miarą wydajności klasyfikatorów, szczególnie gdy mamy do czynienia z niezbalansowanymi danymi (to znaczy, gdy niektóre klasy są znacznie częstsze niż inne)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix\n",
    "\n",
    " * Aby obliczyć **Confusion Matrix**, najpierw trzeba mieć zestaw prognoz, aby można je było porównać z rzeczywistymi etykietami.\n",
    "\n",
    " * Możesz przewidywać na zestawie testowym (test set), ale na razie trzymaj go nietknięty (pamiętaj, że chcesz użyć zestawu testowego tylko na samym końcu projektu, gdy masz klasyfikator, który jesteś gotowy do uruchomienia). \n",
    " * Zamiast tego możesz użyć funkcji \n",
    " ```pyrthon\n",
    " cross_val_predict()\n",
    " ```\n",
    " \n",
    " * Podobnie jak funkcja **cross_val_score()** funkcja **cross_val_predict()** wykonuje **K-fold cross-validation**, ale zamiast zwracać wyniki oceny, zwraca predykcje wykonane dla każdego testu. \n",
    "\n",
    " * Oznacza to, że uzyskujesz czystą prognozę dla każdej instancji w zbiorze treningowym (\"czysta\" oznacza, że przewidywanie jest dokonywane przez model, który nigdy nie widział danych podczas treningu). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_train_pred = cross_val_predict(clf, X_train, y_train_5, cv=3)\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix\n",
    "Do **confusion_matrix()** wystarczy przekazać etykiety docelowe (y_train_5) i przewidywane klasy (y_train_pred):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Każdy rząd **confusion_matrix** reprezentuje rzeczywistą klasę, podczas gdy każda kolumna reprezentuje przewidywaną klasę.\n",
    "* Pierwszy rząd tej macierzy uwzględnia obrazy inne niż 5 (klasa negatywna): \n",
    "  * 53 272 z nich zostało poprawnie sklasyfikowanych jako non-5 (**true negatives**), \n",
    "  *  1 307 zostało błędnie sklasyfikowane jako 5s (**false positives**). \n",
    "* Drugi rząd uwzględnia obrazy 5s (klasa pozytywna): \n",
    "  * 1 077 zostały błędnie sklasyfikowane jako nie-5 (**false negatives**), \n",
    "  * 4 344 zostały poprawnie sklasyfikowane jako 5s (**true positives**). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idealny klasyfikator miałby tylko **true positives** i **true negatives**, więc jego **confusion_matrix** miałaby niezerowe wartości tylko na swojej głównej przekątnej (od lewej górnej do prawej dolnej):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_perfect_predictions = y_train_5\n",
    "confusion_matrix(y_train_5, y_train_perfect_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix** dostarcza wielu informacji, ale bardziej przydatne sa miary numeryczne. \n",
    "\n",
    " * **precision** to dokładność pozytywnych przewidywań -- precyzja klasyfikatora:\n",
    "\n",
    "$$\n",
    "precision = \\frac{TP}{TP + FP}\n",
    "$$\n",
    " \n",
    " * Prostym sposobem na uzyskanie doskonałej precyzji jest wykonanie jednej pozytywnej prognozy i zapewnienie jej poprawności (precision = 1/1 = 100%). \n",
    " * Nie byłoby to zbyt użyteczne, ponieważ klasyfikator zignorowałby wszystkie oprócz jednego pozytywnego wystąpienia. \n",
    " * Precyzję zwykle stosuje się wraz z inną miarą o nazwie **recall**, zwaną również czułością **sensitivity**.\n",
    " * **recall** jest to stosunek pozytywnych instancji, które są poprawnie wykrywane przez klasyfikator \n",
    " \n",
    "$$\n",
    "recall = \\frac{TP}{TP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "print(precision_score(y_train_5, y_train_pred)) \n",
    "print(recall_score(y_train_5, y_train_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teraz widzimy, że detektor piątek nie wygląda tak dobrze. \n",
    "\n",
    "* Kiedy twierdzi, że obraz reprezentuje 5, jest to poprawne tylko w 79% przypadków. \n",
    "* Co więcej, wykrywa tylko 81% z 5s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 score\n",
    "\n",
    "\n",
    " * Często wygodnie jest łączyć **precision** i **recall** w pojedynczą metrykę zwaną **F1 score** w szczególności jeśli potrzebujesz prostego sposobu na porównanie dwóch klasyfikatorów. \n",
    "\n",
    " * Wynik **F1  score** jest średnią harmoniczną  **precision** i **recall**. \n",
    " \n",
    "$$\n",
    "F_1 = \\frac{2}{ \\frac{1}{precision} + \\frac{1}{recall} } = 2\\frac{precision * recall}{precision + recall}\n",
    "= \\frac{TP}{TP + \\frac{FN+FP}{2}}\n",
    "$$\n",
    "\n",
    " * Podczas gdy klasyczna środek traktuje wszystkie wartości w równym stopniu **średnia harmoniczna** nadaje znacznie większą wagę mniejszym wartością. \n",
    "\n",
    " * W rezultacie klasyfikator uzyska wysoki wynik **F1 score** jeśli zarówno  **precision** i **recall** będą wysokie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_train_5, y_train_pred))#, target_names=[\"not 5\", \"5\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision/recall tradeoff\n",
    "\n",
    " * Wynik **F1 score** faworyzuje klasyfikatory, które mają podobną **precision** i **recall**. \n",
    "\n",
    " * To nie zawsze jest to, czego potrzebujesz: w niektórych kontekstach dbamy głównie o **precision**, a w innych kontekstach naprawdę zależy nam na **recall**. \n",
    " \n",
    " * Na przykład, jeśli wytrenowałeś klasyfikator do wykrywania filmów, które są bezpieczne dla dzieci, prawdopodobnie wolałbyś klasyfikator, który odrzuca wiele dobrych filmów (**low recall**), ale zachowuje tylko te bezpieczne (**high precision**). \n",
    " \n",
    " * Z drugiej strony, przypuśćmy, że szkolisz klasyfikator, który wykrywa kieszonkowców na kamerach z monitoringu: prawdopodobnie dobrze jest jeśli klasyfikator ma niskie **precision** 30%, o ile ma 99% **recall** (na pewno strażnicy dostaną kilka fałszywych alarmów, ale prawie wszyscy złodzieje zostaną złapani).\n",
    " \n",
    " * Niestety nie możesz minimalizować ich obu: zwiększenie **precision** zmniejsza **recall** i na odwrót. \n",
    " Nazywa się to **precision/recall tradeoff**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Przykład\n",
    "\n",
    "* Aby zrozumieć ten kompromis przyjrzyjmy się, w jaki sposób SGDClassifier podejmuje decyzje dotyczące klasyfikacji. \n",
    "* Dla każdej instancji oblicza funkcję decyzyjną, a jeśli ten wynik jest większy niż z góry ustawiny próg, przydziela instancję do klasy pozytywnej lub negatywnej. \n",
    "* Rysunek pokazuje kilka cyfr umieszczonych od najniższego wyniku po lewej do najwyższego wyniku po prawej stronie.\n",
    "\n",
    "   * Przypuśćmy, że próg decyzyjny znajduje się przy centralnej strzałce (pomiędzy dwoma piątkami): \n",
    "        \n",
    "        * znajdziesz 4 prawdziwe piatki po prawej stronie tego progu i jedną fałszywą . Dlatego przy tym progu **precision** wynosi 80\\% (4 na 5). \n",
    "        * Ale z 6 rzeczywistych 5, klasyfikator wykrywa tylko 4, więc **recall** wynosi 67% (4 z 6). \n",
    "   \n",
    "   * Teraz, jeśli podniesiesz próg (przesuń go do strzałki po prawej) szusty wynik false positive staje się becomes a true negative, tym samym zwiększając **precision** (do 100% w tym przypadku), ale jeden wynik true positive staje się false negative zmniejszając **recall** do 50%. \n",
    "   \n",
    "   * I odwrotnie, obniżenie progu zwiększa zapamiętywanie i zmniejsza precyzję.\n",
    "   \n",
    "Scikit-Learn nie pozwala bezpośrednio ustawić progu, ale daje dostęp do funkcji decyzyjnej, którą wykorzystuje do prognozowania. Zamiast wywoływania metody **predict()** klasyfikatora, można wywołać metodę **decision_function()**, która zwraca wartość dla każdej instancji, a następnie utworzyć prognozy na podstawie tych wyników, używając dowolnego progu, który chcesz:\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_scores = clf.predict_proba([some_digit])\n",
    "y_scores = clf.decision_function([some_digit])\n",
    "y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "y_some_digit_pred = (y_scores > threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_some_digit_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LogisticRegression** używa progu równego 0, więc poprzedni kod zwraca ten sam wynik, co metoda **predict()** (tj. True). \n",
    "\n",
    "Podnieśmy próg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 200000\n",
    "y_some_digit_pred = (y_scores > threshold)\n",
    "y_some_digit_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jak więc zdecydować, który próg wykorzystać? \n",
    "\n",
    "W tym celu trzeba uzyskać wyniki wszystkich instancji w zestawie szkoleniowym za pomocą funkcji **cross_val_predict()** ale tym razem określając, że ma ona zwrócić wyniki funkcji decyzyjnej zamiast przewidywań:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(clf, X_train, y_train_5, cv=3, method=\"decision_function\")\n",
    "print(y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teraz za pomocą tych wyników można obliczyć **precision** i **recall** dla wszystkie możliwych progów, używając funkcji **precision_recall_curve()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(clf, X_train, y_train_5, cv=3, method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hack to work around issue #9589 in Scikit-Learn 0.19.0\n",
    "if y_scores.ndim == 2:\n",
    "    y_scores = y_scores[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "    plt.xlabel(\"Threshold\", fontsize=16)\n",
    "    plt.legend(loc=\"upper left\", fontsize=16)\n",
    "#     plt.ylim([0, 1])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "# plt.xlim([-700000, 700000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Innym sposobem na rozwiązanie **precision/recall tradeoff** jest przeanalizowanie wykresu **precision vs recall**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_precision_vs_recall(precisions, recalls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Widać, że **precision** zaczyna gwałtownie spadać w okolicach 90%. \n",
    " * Prawdopodobnie będziesz chciał wybrać kompromis pomiędzy **precision** i **recall** tuż przed tym spadkiem.\n",
    " * Na przykład około 80% **recall**.\n",
    " \n",
    "Załóżmy więc, że zdecydujesz się dążyć do 95% **precision**. Wystarczy nieco powiększyć pierwszy wykres i odczytać, że musisz użyć progu około 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_90 = (y_scores > 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(precision_score(y_train_5, y_train_pred_90))\n",
    "print(recall_score(y_train_5, y_train_pred_90))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Świetnie, masz klasyfikator z dokładnością 90% (lub wystarczająco blisko)! Jak widać, dość łatwo jest utworzyć klasyfikator z praktycznie dowolną precyzją: \n",
    " * wystarczy ustawić odpowiednio wysoki próg i gotowe. \n",
    " \n",
    "Klasyfikator o wysokiej dokładności nie jest zbyt przydatny, jeśli jego **recall** jest zbyt niskie!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curve\n",
    "\n",
    "http://arogozhnikov.github.io/2015/10/05/roc-curve.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate', fontsize=16)\n",
    "    plt.ylabel('True Positive Rate', fontsize=16)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  **Receiver operating characteristic (ROC) ** jest kolejnym powszechnym narzędziem stosowanym w klasyfikatorach binarnych. \n",
    "* Jest to bardzo podobne do krzywej **precision/recall curve** ale zamiast opisywać relację między **precision** a **recall** opisuje\n",
    "\n",
    " ### sensitivity (recall) vs 1-specificity\n",
    "\n",
    "gdzie\n",
    "\n",
    "$$\n",
    "specificity = \\frac{TN}{FP+TN}\n",
    "$$\n",
    "$$\n",
    "recall = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "* Ponownie jak wcześniej: im wyższe **recall** tym więcej fałszywych alarmów **specificity**.\n",
    "* Linia przerywana przedstawia krzywą ROC czysto losowego klasyfikatora.\n",
    "* Dobry klasyfikator daje krzywą jak najdalszą od tej.\n",
    "\n",
    "\n",
    "### Jednym ze sposobów porównywania klasyfikatorów jest pomiar **area under the curve (AUC)**. \n",
    "\n",
    "* Idealny klasyfikator będzie miał ROC AUC równe 1\n",
    "* podczas gdy klasyfikator czysto losowy będzie miał ROC AUC równe 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_auc_score(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zad. \n",
    "\n",
    "Nauczmy regresję logistyczną i SVM (domyślne parametry) i porównajmy krzywe ROC i wynik ROC AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf_svc = LinearSVC(random_state=42)\n",
    "clf_svc.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_lr = LogisticRegression(random_state=42)\n",
    "clf_lr.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_lr = clf_lr.decision_function(X_train)\n",
    "fpr_lr, tpr_lr, thresholds_lr = roc_curve(y_train_5, y_scores_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_sv = clf_svc.decision_function(X_train)\n",
    "fpr_sv, tpr_sv, thresholds_sv = roc_curve(y_train_5, y_scores_sv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plot_roc_curve(fpr_lr, tpr_lr, \"LogisticRegression\")\n",
    "plot_roc_curve(fpr_sv, tpr_sv, \"LinearSVC\")\n",
    "plt.legend(loc=\"lower right\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc_auc_score(y_train_5, y_scores_lr))\n",
    "print(roc_auc_score(y_train_5, y_scores_sv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
